# Similar to examples/wiki_search/rl.toml, but
# - 300 instead of 500 steps (short runtime)
# - No LoRA

inference_gpu_ids = [0,1,2,3]
trainer_gpu_ids = [4,5,6,7]

max_steps = 300

[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

[orchestrator]
batch_size = 512
rollouts_per_example = 16
seq_len = 4096

[orchestrator.sampling]
max_tokens = 512

[[orchestrator.env]]
id = "primeintellect/wiki-search"
name = "wiki-search"

[inference.model]
enable_auto_tool_choice = true
tool_call_parser = "hermes"

[trainer] # Default trainer config
