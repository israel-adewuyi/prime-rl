# Run on 8xH200
# Similar to configs/acereason_math/stage1.toml, but
# - 300 instead of 500 steps
# - No checkpointing
# - No evals
inference_gpu_ids = [0,1,2,3]
trainer_gpu_ids = [4,5,6,7]

max_steps = 300

[model]
name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

[orchestrator]
batch_size = 1024
seq_len = 8192
rollouts_per_example = 8

[orchestrator.sampling]
temperature = 0.6
max_tokens = 8192

[[orchestrator.env]]
id = "primeintellect/single-turn-math"
name = "acereason-math"
args = { dataset_name = "nvidia/AceReason-Math", dataset_subset = "default", question_key = "problem" }

[trainer]

[inference]